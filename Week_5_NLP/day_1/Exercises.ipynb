{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/patash/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Why', ',', 'sometimes', 'I', '`', 've', 'believed', 'as', 'many', 'as', '6', 'impossible', 'things', 'before', 'breakfast', '?']\n",
      "[',', 'sometimes', 'I', '`', 've', 'believed']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "doc = 'Why, sometimes I`ve believed as many as 6 impossible things before breakfast?'\n",
    "tokens = word_tokenize(doc)\n",
    "print(tokens)\n",
    "\n",
    "span = tokens[1:7]  # Indices des mots\n",
    "print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['Linguistics', 'and', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'are', 'closely', 'linked', '.', 'Linguistics', 'is', 'the', 'scientific', 'study', 'of', 'language', ',', 'encompassing', 'its', 'structure', ',', 'meaning', ',', 'and', 'context', '.', 'It', 'provides', 'foundational', 'knowledge', 'about', 'language', 'syntax', ',', 'semantics', ',', 'pragmatics', ',', 'and', 'phonetics', '.', 'NLP', 'applies', 'this', 'linguistic', 'knowledge', 'in', 'computational', 'algorithms', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'By', 'leveraging', 'linguistic', 'principles', ',', 'NLP', 'seeks', 'to', 'bridge', 'the', 'gap', 'between', 'human', 'communication', 'and', 'computer', 'understanding', ',', 'enabling', 'tasks', 'like', 'translation', ',', 'sentiment', 'analysis', ',', 'and', 'voice', 'recognition', '.']\n",
      "Filtered Tokens: ['Linguistics', 'Natural', 'Language', 'Processing', 'NLP', 'closely', 'linked', 'Linguistics', 'scientific', 'study', 'language', 'encompassing', 'structure', 'meaning', 'context', 'provides', 'foundational', 'knowledge', 'language', 'syntax', 'semantics', 'pragmatics', 'phonetics', 'NLP', 'applies', 'linguistic', 'knowledge', 'computational', 'algorithms', 'enable', 'computers', 'understand', 'interpret', 'generate', 'human', 'language', 'leveraging', 'linguistic', 'principles', 'NLP', 'seeks', 'bridge', 'gap', 'human', 'communication', 'computer', 'understanding', 'enabling', 'tasks', 'like', 'translation', 'sentiment', 'analysis', 'voice', 'recognition']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/patash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stopwords Removal + syntaxe \n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "doc = '''Linguistics and Natural Language Processing (NLP) are closely linked. \\n Linguistics is the scientific study of language, encompassing its structure, meaning, and context. \\n It provides foundational knowledge about language syntax, semantics, pragmatics, and phonetics. \\n NLP applies this linguistic knowledge in computational algorithms to enable computers to understand, interpret, and generate human language. By leveraging linguistic principles, NLP seeks to bridge the gap between human communication and computer understanding, enabling tasks like translation, sentiment analysis, and voice recognition.'''\n",
    "\n",
    "# Tokenisation\n",
    "tokens = word_tokenize(doc)\n",
    "\n",
    "# Nettoyage: suppression des caractères non alphabétiques\n",
    "tokens_cleaned = [re.sub(r'\\W+', '', token) for token in tokens if re.sub(r'\\W+', '', token) != '']\n",
    "\n",
    "# Filtrage des stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens_cleaned if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Filtered Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['linguist', 'natur', 'languag', 'process', 'nlp', 'close', 'link', 'linguist', 'scientif', 'studi', 'languag', 'encompass', 'structur', 'mean', 'context', 'provid', 'foundat', 'knowledg', 'languag', 'syntax', 'semant', 'pragmat', 'phonet', 'nlp', 'appli', 'linguist', 'knowledg', 'comput', 'algorithm', 'enabl', 'comput', 'understand', 'interpret', 'gener', 'human', 'languag', 'leverag', 'linguist', 'principl', 'nlp', 'seek', 'bridg', 'gap', 'human', 'commun', 'comput', 'understand', 'enabl', 'task', 'like', 'translat', 'sentiment', 'analysi', 'voic', 'recognit']\n"
     ]
    }
   ],
   "source": [
    "# Stemming - pour retirer la racine des mots\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.11/site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Lemmatized:  ['Linguistics', 'Natural', 'Language', 'Processing', 'NLP', 'closely', 'link', 'Linguistics', 'scientific', 'study', 'language', 'encompass', 'structure', 'mean', 'context', 'provide', 'foundational', 'knowledge', 'language', 'syntax', 'semantic', 'pragmatic', 'phonetic', 'NLP', 'apply', 'linguistic', 'knowledge', 'computational', 'algorithm', 'enable', 'computer', 'understand', 'interpret', 'generate', 'human', 'language', 'leverage', 'linguistic', 'principle', 'NLP', 'seek', 'bridge', 'gap', 'human', 'communication', 'computer', 'understanding', 'enable', 'task', 'like', 'translation', 'sentiment', 'analysis', 'voice', 'recognition']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization - transformer les mots en forme initiale/basique: suis - être\n",
    "!pip install spacy\n",
    "import spacy\n",
    "\n",
    "#load spaCy's English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(\" \".join(filtered_tokens))\n",
    "\n",
    "lemmatized = [token.lemma_ for token in doc]\n",
    "print('Lemmatized: ', lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed: ['charl', 'lutwidg', 'dodgson', ',', 'better', 'known', 'by', 'hi', 'pen', 'name', 'lewi', 'carrol', ',', 'wa', 'an', 'english', 'author', ',', 'poet', ',', 'mathematician', 'and', 'photograph', '.', 'hi', 'most', 'notabl', 'work', 'are', 'alic', \"'s\", 'adventur', 'in', 'wonderland', '(', '1865', ')', 'and', 'it', 'sequel', 'through', 'the', 'looking-glass', '(', '1871', ')', '.']\n",
      "Lemmatized:  ['Charles', 'Lutwidge', 'Dodgson', ',', 'well', 'know', 'by', 'his', 'pen', 'name', 'Lewis', 'Carroll', ',', 'be', 'an', 'english', 'author', ',', 'poet', ',', 'mathematician', 'and', 'photographer', '.', 'his', 'most', 'notable', 'work', 'be', 'Alice', \"'s\", 'Adventures', 'in', 'Wonderland', '(', '1865', ')', 'and', 'its', 'sequel', 'through', 'the', 'Looking', '-', 'Glass', '(', '1871', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = '''Charles Lutwidge Dodgson, better known by his pen name Lewis Carroll, was an English author, poet, mathematician and photographer. His most notable works are Alice's Adventures in Wonderland (1865) and its sequel Through the Looking-Glass (1871).'''\n",
    "\n",
    "tokens = word_tokenize(doc)\n",
    "\n",
    "# Initialiser le stemmer\n",
    "stemmer = PorterStemmer()\n",
    "# Appliquer le stemming aux tokens\n",
    "stemmed = [stemmer.stem(word) for word in tokens]\n",
    "print('Stemmed:' ,stemmed)\n",
    "\n",
    "# Charger le modèle de langue anglaise de spaCy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Traiter le texte avec spaCy\n",
    "doc = nlp(doc)\n",
    "\n",
    "# Filtrer les mots vides et les caractères non alphabétiquesc\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_doc = [word for word in doc if word not in stopwords.words('english')]\n",
    "\n",
    "# Appliquer la lemmatisation\n",
    "lemmatized = [token.lemma_ for token in filtered_doc]\n",
    "print('Lemmatized: ', lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/patash/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Charles, POS: NNP\n",
      "Word: Lutwidge, POS: NNP\n",
      "Word: Dodgson, POS: NNP\n",
      "Word: ,, POS: ,\n",
      "Word: better, POS: JJR\n",
      "Word: known, POS: VBN\n",
      "Word: by, POS: IN\n",
      "Word: his, POS: PRP$\n",
      "Word: pen, POS: JJ\n",
      "Word: name, POS: NN\n",
      "Word: Lewis, POS: NNP\n",
      "Word: Carroll, POS: NNP\n",
      "Word: ,, POS: ,\n",
      "Word: was, POS: VBD\n",
      "Word: an, POS: DT\n",
      "Word: English, POS: JJ\n",
      "Word: author, POS: NN\n",
      "Word: ,, POS: ,\n",
      "Word: poet, POS: NN\n",
      "Word: ,, POS: ,\n",
      "Word: mathematician, POS: JJ\n",
      "Word: and, POS: CC\n",
      "Word: photographer, POS: NN\n",
      "Word: ., POS: .\n",
      "Word: His, POS: PRP$\n",
      "Word: most, POS: RBS\n",
      "Word: notable, POS: JJ\n",
      "Word: works, POS: NNS\n",
      "Word: are, POS: VBP\n",
      "Word: Alice, POS: NNP\n",
      "Word: 's, POS: POS\n",
      "Word: Adventures, POS: NNS\n",
      "Word: in, POS: IN\n",
      "Word: Wonderland, POS: NNP\n",
      "Word: (, POS: (\n",
      "Word: 1865, POS: CD\n",
      "Word: ), POS: )\n",
      "Word: and, POS: CC\n",
      "Word: its, POS: PRP$\n",
      "Word: sequel, POS: NN\n",
      "Word: Through, POS: IN\n",
      "Word: the, POS: DT\n",
      "Word: Looking-Glass, POS: NNP\n",
      "Word: (, POS: (\n",
      "Word: 1871, POS: CD\n",
      "Word: ), POS: )\n",
      "Word: ., POS: .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# POS (Part Of Speech) - NNP - Nom propre singulier\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "\n",
    "tagged = pos_tag(tokens)\n",
    "for word, tag in tagged:\n",
    "    print(f'Word: {word}, POS: {tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Apple/NNP)\n",
      "  is/VBZ\n",
      "  looking/VBG\n",
      "  at/IN\n",
      "  buying/VBG\n",
      "  U.K./NNP\n",
      "  startup/NN\n",
      "  for/IN\n",
      "  $/$\n",
      "  1/CD\n",
      "  billion/CD)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/patash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/patash/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/patash/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/patash/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "from nltk import pos_tag, ne_chunk, word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "doc = 'Apple is looking at buying U.K. startup for $1 billion'\n",
    "tokens = word_tokenize(doc)\n",
    "\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "entities = ne_chunk(tagged)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({',': 1, 'sometimes': 1, '`': 1, 'believed': 1, 'many': 1, '6': 1, 'impossible': 1, 'things': 1, 'breakfast': 1, '.': 1, 'goes': 1, 'shawl': 1, '!': 1})\n"
     ]
    }
   ],
   "source": [
    "# Vectorization\n",
    "# Bag of Words\n",
    "from collections import Counter\n",
    "doc = 'Why, sometimes I`ve believed as many as 6 impossible things before breakfast. There goes the shawl again!'\n",
    "tokens = word_tokenize(doc)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in tokens]\n",
    "filtered_words = [word for word in tokens if not word.lower() in stopwords.words('english')]\n",
    "\n",
    "bow = Counter(filtered_words)\n",
    "print(bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['4th' 'and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Vectorized representation:\n",
      " [[0 0 1 1 1 0 0 1 0 1]\n",
      " [0 0 2 0 1 0 1 1 0 1]\n",
      " [0 1 0 0 1 1 0 1 1 1]\n",
      " [1 0 0 0 1 1 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Créer le vocabulaire de plusiers documents \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = ['This is the first document.','This document is the second document.','And this is the third one.', 'This one is the 4th']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
    "print(\"Vectorized representation:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['books' 'coffee' 'in' 'is' 'love' 'morning' 'reading' 'refreshing' 'the']\n",
      "TF-IDF Matrix:\n",
      "[[0.68091856 0.         0.         0.         0.51785612 0.\n",
      "  0.51785612 0.         0.        ]\n",
      " [0.         0.         0.44036207 0.44036207 0.         0.3349067\n",
      "  0.3349067  0.44036207 0.44036207]\n",
      " [0.         0.68091856 0.         0.         0.51785612 0.51785612\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Step 1: Load the Documents\n",
    "documents = [\n",
    "    \"I love reading books.\",\n",
    "    \"Reading in the morning is refreshing.\",\n",
    "    \"I love morning coffee.\"\n",
    "]\n",
    "\n",
    "# Step 2: Apply TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 3: Analyze the Output\n",
    "# Print the vocabulary (unique words)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the TF-IDF values for each document\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two roads diverged yellow wood sorry could travel one traveler long stood looked one far could bent undergrowth', 'Whose woods think know house village though see stopping watch woods fill snow', 'keep head losing blaming trust men doubt make allowance doubting']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "processed_poems = []\n",
    "with open('/Users/patash/PSTB/Week_5/day_1/famous_poems.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#cleaning stopwords\n",
    "for poem in data:\n",
    "    tokens = word_tokenize(poem['text'])\n",
    "    filtered_tokens = [word for word in tokens if not word.lower() in stopwords.words('english') and word.isalpha()]\n",
    "    processed_poems.append(' '.join(filtered_tokens))\n",
    "\n",
    "print(processed_poems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.11/site-packages (4.3.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n",
      "Collecting FuzzyTM>=0.4.0 (from gensim)\n",
      "  Downloading FuzzyTM-2.0.9-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (2.1.4)\n",
      "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading pyFUME-0.3.4-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3)\n",
      "Collecting scipy>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.10.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (100 kB)\n",
      "Collecting numpy>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting simpful==2.12.0 (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting fst-pso==1.8.1 (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandas (from FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading pandas-1.5.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting miniful (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim)\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Downloading FuzzyTM-2.0.9-py3-none-any.whl (31 kB)\n",
      "Downloading pyFUME-0.3.4-py3-none-any.whl (60 kB)\n",
      "Downloading numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-macosx_12_0_arm64.whl (28.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.7/28.7 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading simpful-2.12.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20430 sha256=ecc1c581859b46e6cc2e35c0a4fb1c5d222007ac4f0689198f85c0b0f1280d28\n",
      "  Stored in directory: /Users/patash/Library/Caches/pip/wheels/69/f5/e5/18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
      "  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3507 sha256=88bf39e76067b3d70b56574e3a6794d1f9f90ed0bf58d0ef9118f4a0cbf8f1e6\n",
      "  Stored in directory: /Users/patash/Library/Caches/pip/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: numpy, scipy, pandas, simpful, miniful, fst-pso, pyfume, FuzzyTM\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.0\n",
      "    Uninstalling scipy-1.10.0:\n",
      "      Successfully uninstalled scipy-1.10.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.4\n",
      "    Uninstalling pandas-2.1.4:\n",
      "      Successfully uninstalled pandas-2.1.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed FuzzyTM-2.0.9 fst-pso-1.8.1 miniful-0.0.6 numpy-1.24.4 pandas-1.5.3 pyfume-0.3.4 scipy-1.10.1 simpful-2.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/patash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk gensim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "nltk.download('punkt')\n",
    "\n",
    "dataset = api.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.5899737477302551)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Word2Vec(sentences=dataset, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "example_result = model.wv.most_similar(positive=['woman', 'king'], negative=['savory'], topn=1)\n",
    "print(example_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: kings, Similarity: 0.7138046622276306\n",
      "Word: queen, Similarity: 0.6510956287384033\n",
      "Word: monarch, Similarity: 0.6413194537162781\n",
      "Word: crown_prince, Similarity: 0.6204219460487366\n",
      "Word: prince, Similarity: 0.6159993410110474\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pre-trained Word2Vec model (Google News 300D)\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Find the top 5 most similar words to \"king\"\n",
    "similar_words = word2vec_model.most_similar(\"king\", topn=5)\n",
    "\n",
    "# Print the results\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"Word: {word}, Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPAM\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"/Users/patash/PSTB/Week_5/day_1/spam.csv\", encoding=\"latin-1\")\n",
    "df = df[[\"v1\", \"v2\"]]  # Keep only the label and text columns\n",
    "df.columns = [\"label\", \"text\"]\n",
    "\n",
    "# Convert labels to binary (spam = 1, ham = 0)\n",
    "df[\"label\"] = df[\"label\"].map({\"spam\": 1, \"ham\": 0})\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      0  Go until jurong point, crazy.. Available only ...\n",
      "1      0                      Ok lar... Joking wif u oni...\n",
      "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      0  U dun say so early hor... U c already then say...\n",
      "4      0  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Extraction With TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert text to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9668161434977578\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       965\n",
      "           1       1.00      0.75      0.86       150\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.88      0.92      1115\n",
      "weighted avg       0.97      0.97      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Train A Naive Bayes Classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train the model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
