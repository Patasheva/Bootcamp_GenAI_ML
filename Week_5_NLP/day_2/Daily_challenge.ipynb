{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "inputs = torch.tensor(\n",
    "[\n",
    "    [0.43, 0.15, 0.89], # your\n",
    "    [0.55, 0.87, 0.66], # journey\n",
    "    [0.57, 0.85, 0.64], # starts\n",
    "    [0.22, 0.58, 0.33], # with\n",
    "    [0.77, 0.25, 0.10], # one\n",
    "    [0.05, 0.80, 0.55] # step\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Vector (starts): tensor([0.5700, 0.8500, 0.6400])\n"
     ]
    }
   ],
   "source": [
    "# Query vector \n",
    "query_vector = inputs[2]  # Index 2 = \"starts\"\n",
    "\n",
    "print(\"Query Vector (starts):\", query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 vector: tensor([0.4300, 0.1500, 0.8900])\n",
      "Word 1 vector: tensor([0.5500, 0.8700, 0.6600])\n",
      "Word 2 vector: tensor([0.5700, 0.8500, 0.6400])\n",
      "Word 3 vector: tensor([0.2200, 0.5800, 0.3300])\n",
      "Word 4 vector: tensor([0.7700, 0.2500, 0.1000])\n",
      "Word 5 vector: tensor([0.0500, 0.8000, 0.5500])\n",
      "\n",
      "Attention Scores: tensor([0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605])\n"
     ]
    }
   ],
   "source": [
    "# Attention scores\n",
    "attn_scores_2 = torch.zeros(len(inputs))  \n",
    "for i, x_i in enumerate(inputs):\n",
    "  print(f\"Word {i} vector: {x_i}\")\n",
    "  attn_scores_2[i] = torch.dot(x_i, query_vector)\n",
    "\n",
    "print(\"\\nAttention Scores:\", attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights (Softmax): tensor([0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565])\n"
     ]
    }
   ],
   "source": [
    "# Attention weights = attention scores transformed in probability (softmax)\n",
    "attn_weights = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention Weights (Softmax):\", attn_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector: tensor([0.4431, 0.6496, 0.5671])\n"
     ]
    }
   ],
   "source": [
    "# Context vector\n",
    "context_vector = torch.sum(attn_weights[:, None] * inputs, dim=0)\n",
    "print(\"Context Vector:\", context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores:\n",
      " tensor([[ 2.6427,  4.0432,  4.0849,  2.0004,  3.6882,  1.7522],\n",
      "        [ 1.1121,  3.1939,  3.2997,  1.5280,  4.2758,  0.6275],\n",
      "        [ 1.1278,  3.1771,  3.2839,  1.5120,  4.2831,  0.6013],\n",
      "        [ 0.2329,  1.4106,  1.4675,  0.6870,  2.0786,  0.1898],\n",
      "        [ 1.0943,  1.9823,  2.0776,  0.7994,  3.2089, -0.0389],\n",
      "        [ 0.1420,  1.7081,  1.7589,  0.9060,  2.1774,  0.4759]])\n",
      "Attention weights(Softmax):\n",
      " tensor([[0.0765, 0.3104, 0.3237, 0.0403, 0.2177, 0.0314],\n",
      "        [0.0229, 0.1834, 0.2039, 0.0347, 0.5411, 0.0141],\n",
      "        [0.0233, 0.1809, 0.2013, 0.0342, 0.5466, 0.0138],\n",
      "        [0.0604, 0.1962, 0.2077, 0.0952, 0.3827, 0.0579],\n",
      "        [0.0647, 0.1572, 0.1730, 0.0482, 0.5361, 0.0208],\n",
      "        [0.0454, 0.2174, 0.2287, 0.0975, 0.3476, 0.0634]])\n",
      "Context vector:\n",
      " tensor([[ 0.5115,  0.0890, -0.6006],\n",
      "        [ 0.5043,  0.1628, -0.4568],\n",
      "        [ 0.5043,  0.1634, -0.4538],\n",
      "        [ 0.4780,  0.1176, -0.5031],\n",
      "        [ 0.4983,  0.1410, -0.4333],\n",
      "        [ 0.4769,  0.1193, -0.5290]])\n"
     ]
    }
   ],
   "source": [
    "# All words\n",
    "\n",
    "# Création de Q, K et V \n",
    "W_Q = torch.randn(3, 3)  # Matrice des Queries\n",
    "W_K = torch.randn(3, 3)  # Matrice des Keys\n",
    "W_V = torch.randn(3, 3)  # Matrice des Values\n",
    "\n",
    "Q = inputs @ W_Q  \n",
    "K = inputs @ W_K  \n",
    "V = inputs @ W_V  \n",
    "\n",
    "# Attention scores\n",
    "attn_scores = Q @ K.T  \n",
    "\n",
    "# Attention weights\n",
    "attn_weights = torch.nn.functional.softmax(attn_scores, dim=1)\n",
    "\n",
    "# Context vector\n",
    "context_vector = attn_weights @ V\n",
    "\n",
    "print(\"Attention scores:\\n\", attn_scores)\n",
    "print(\"Attention weights(Softmax):\\n\", attn_weights)\n",
    "print(\"Context vector:\\n\", context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 'starts':\n",
      " tensor([-1.9908, -0.8355,  0.3953], grad_fn=<SqueezeBackward4>)\n",
      "Key 'starts':\n",
      " tensor([ 1.5079, -0.2364,  0.5598], grad_fn=<SqueezeBackward4>)\n",
      "Value 'starts':\n",
      " tensor([1.0573, 2.3881, 3.7660], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Matrixes \n",
    "Wq = nn.Parameter(torch.randn(3, 3))  \n",
    "Wk = nn.Parameter(torch.randn(3, 3))  \n",
    "Wv = nn.Parameter(torch.randn(3, 3))  \n",
    "\n",
    "# Inputs[2] - starts \n",
    "input_for_starts = inputs[2] \n",
    "\n",
    "# Q, K, V\n",
    "query_for_starts = torch.matmul(input_for_starts, Wq)  \n",
    "key_for_starts = torch.matmul(input_for_starts, Wk)    \n",
    "value_for_starts = torch.matmul(input_for_starts, Wv)  \n",
    "\n",
    "print(\"Query 'starts':\\n\", query_for_starts)\n",
    "print(\"Key 'starts':\\n\", key_for_starts)\n",
    "print(\"Value 'starts':\\n\", value_for_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 'journey':\n",
      " tensor([-0.2992, -2.3014,  1.2407], grad_fn=<SqueezeBackward4>)\n",
      "Key 'journey':\n",
      " tensor([ 1.1865, -1.3654,  1.7758], grad_fn=<SqueezeBackward4>)\n",
      "Value 'journey':\n",
      " tensor([1.0650, 2.5115, 1.6445], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "# Matrixes \n",
    "Wq = nn.Parameter(torch.randn(3, 3))  \n",
    "Wk = nn.Parameter(torch.randn(3, 3))  \n",
    "Wv = nn.Parameter(torch.randn(3, 3))  \n",
    "\n",
    "# Inputs[1] - journey\n",
    "input_for_journey = inputs[1] \n",
    "\n",
    "# Q, K, V\n",
    "query_for_journey = torch.matmul(input_for_journey, Wq)  \n",
    "key_for_journey = torch.matmul(input_for_journey, Wk)    \n",
    "value_for_journey = torch.matmul(input_for_journey, Wv)  \n",
    "\n",
    "print(\"Query 'journey':\\n\", query_for_journey)\n",
    "print(\"Key 'journey':\\n\", key_for_journey)\n",
    "print(\"Value 'journey':\\n\", value_for_journey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention score ω11:\n",
      " tensor(4.9904, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Attention Score inputs[1][1]\n",
    "attn_score_11 = torch.dot(query_for_journey, key_for_journey)\n",
    "print(\"Attention score ω11:\\n\", attn_score_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores inputs[1]:\n",
      " [tensor(2.8732, grad_fn=<DotBackward0>), tensor(4.9904, grad_fn=<DotBackward0>), tensor(4.9454, grad_fn=<DotBackward0>), tensor(2.8070, grad_fn=<DotBackward0>), tensor(2.7413, grad_fn=<DotBackward0>), tensor(3.4210, grad_fn=<DotBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "# Attention Score inputs[1]\n",
    "attn_scores_1 = []\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    key_vector = torch.matmul(inputs[i], Wk)  \n",
    "    attn_score = torch.dot(query_for_journey, key_vector) \n",
    "    attn_scores_1.append(attn_score)\n",
    "\n",
    "print(\"Attention scores inputs[1]:\\n\", attn_scores_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights inputs[1]:\n",
      " tensor([0.0481, 0.3996, 0.3820, 0.0450, 0.0422, 0.0832])\n"
     ]
    }
   ],
   "source": [
    "# Attention weights inputs[1]\n",
    "softmax_scores = torch.nn.functional.softmax(torch.tensor(attn_scores_1), dim=0)\n",
    "print(\"Attention weights inputs[1]:\\n\", softmax_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector inputs[1]:\n",
      " tensor([0.9860, 2.3239, 1.5093], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Context vector inputs[1]\n",
    "context_vector = torch.zeros(3)\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    value_vector = torch.matmul(inputs[i], Wv) \n",
    "    context_vector += softmax_scores[i] * value_vector  \n",
    "\n",
    "print(\"Context vector inputs[1]:\\n\", context_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores:\n",
      " tensor([[ 0.1635,  1.1155,  1.0418,  0.8855, -0.5797,  1.6267],\n",
      "        [ 1.2884,  2.1836,  2.0885,  1.4387, -0.2142,  2.4290],\n",
      "        [ 1.2990,  2.1314,  2.0412,  1.3901, -0.1591,  2.3316],\n",
      "        [ 0.7533,  1.3436,  1.2815,  0.9019, -0.1994,  1.5454],\n",
      "        [ 1.1244,  0.5905,  0.6147,  0.1229,  0.8776, -0.0793],\n",
      "        [ 0.6938,  1.9217,  1.8086,  1.4200, -0.7379,  2.5677]])\n",
      "\n",
      "Attention Weights:\n",
      " tensor([[0.0778, 0.2016, 0.1873, 0.1602, 0.0370, 0.3361],\n",
      "        [0.0982, 0.2403, 0.2185, 0.1141, 0.0218, 0.3071],\n",
      "        [0.1049, 0.2411, 0.2203, 0.1149, 0.0244, 0.2945],\n",
      "        [0.1212, 0.2186, 0.2054, 0.1406, 0.0467, 0.2675],\n",
      "        [0.2750, 0.1613, 0.1652, 0.1010, 0.2149, 0.0825],\n",
      "        [0.0614, 0.2097, 0.1873, 0.1270, 0.0147, 0.4000]])\n",
      "\n",
      "Context Vectors:\n",
      " tensor([[-0.5121,  0.8552,  0.5592],\n",
      "        [-0.5482,  0.9291,  0.5807],\n",
      "        [-0.5537,  0.9489,  0.5807],\n",
      "        [-0.5537,  0.9813,  0.5669],\n",
      "        [-0.6461,  1.4179,  0.5257],\n",
      "        [-0.5000,  0.7713,  0.5676]])\n"
     ]
    }
   ],
   "source": [
    "# Weight Parameters for All Inputs\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Matrixes\n",
    "W_Q = torch.randn(3, 3)  \n",
    "W_K = torch.randn(3, 3)  \n",
    "W_V = torch.randn(3, 3)  \n",
    "\n",
    "Q = inputs @ W_Q  \n",
    "K = inputs @ W_K  \n",
    "V = inputs @ W_V  \n",
    "\n",
    "# Attention scores\n",
    "attention_scores = Q @ K.T \n",
    "\n",
    "# Attention weights\n",
    "attention_weights = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "# Context vector\n",
    "context_vectors = attention_weights @ V\n",
    "\n",
    "print(\"Attention Scores:\\n\", attention_scores)\n",
    "print(\"\\nAttention Weights:\\n\", attention_weights)\n",
    "print(\"\\nContext Vectors:\\n\", context_vectors)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
