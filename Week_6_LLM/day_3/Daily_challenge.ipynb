{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1\n",
    "1. Why Evaluating LLMs is More Complex Than Traditional Software\n",
    "LLMs generate probabilistic outputs, making their behavior less predictable than traditional software. They also deal with the complexities of human language, which involves nuance, context, and subjectivity, unlike deterministic software systems.\n",
    "\n",
    "2. Key Reasons for Evaluating an LLM’s Safety\n",
    "LLM safety must be evaluated to prevent bias, harmful content, and misinformation. It also ensures the model adheres to ethical guidelines and maintains user trust by avoiding offensive or dangerous outputs.\n",
    "\n",
    "3. How Adversarial Testing Contributes to LLM Improvement\n",
    "Adversarial testing helps identify weaknesses, edge cases, and vulnerabilities in LLMs by exposing them to challenging inputs. This improves robustness, accuracy, and the model’s ability to handle unexpected scenarios.\n",
    "\n",
    "4. Limitations of Automated Evaluation Metrics vs. Human Evaluation\n",
    "Automated metrics are fast but lack the depth to assess context, creativity, or ethical considerations. Human evaluation offers more nuanced insights but is slower and more subjective, making a combination of both essential for effective evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.9.1)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.11/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from rouge-score) (1.26.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from rouge-score) (1.16.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=7221c040de2736e735da7cf69470853d19f7cdb11932373c007b9b2ec86e21b4\n",
      "  Stored in directory: /Users/patash/Library/Caches/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "BLEU Score: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/patash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2\n",
    "!pip install nltk rouge-score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Référence et texte généré\n",
    "reference_text = [\"Despite the increasing reliance on artificial intelligence in various industries, human oversight remains essential to ensure ethical and effective implementation.\"]\n",
    "generated_text = [\"Although AI is being used more in industries, human supervision is still necessary for ethical and effective application.\"]\n",
    "\n",
    "# Calcul du score BLEU\n",
    "bleu_score = sentence_bleu([reference_text], generated_text)\n",
    "print(f\"BLEU Score: {bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: Score(precision=0.47058823529411764, recall=0.3333333333333333, fmeasure=0.39024390243902435)\n",
      "ROUGE-2: Score(precision=0.1875, recall=0.13043478260869565, fmeasure=0.15384615384615383)\n",
      "ROUGE-L: Score(precision=0.35294117647058826, recall=0.25, fmeasure=0.2926829268292683)\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "reference_text = \"In the face of rapid climate change, global initiatives must focus on reducing carbon emissions and developing sustainable energy sources to mitigate environmental impact.\"\n",
    "generated_text = \"To counteract climate change, worldwide efforts should aim to lower carbon emissions and enhance renewable energy development.\"\n",
    "\n",
    "# Initialisation du calculateur ROUGE\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calcul du score ROUGE\n",
    "scores = scorer.score(reference_text, generated_text)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"ROUGE-1: {scores['rouge1']}\") # Unigrammes\n",
    "print(f\"ROUGE-2: {scores['rouge2']}\") # Bigrams\n",
    "print(f\"ROUGE-L: {scores['rougeL']}\") # Longest Common Subsequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU et ROUGE sont limités car ils se basent sur des correspondances exactes de mots ou de n-grammes, sans tenir compte du sens ou des synonymes. Ils ne capturent pas bien la créativité, la fluidité et la pertinence contextuelle du texte généré. Des alternatives comme BERTScore, l'évaluation humaine et des métriques sémantiques basées sur des embeddings offrent une meilleure évaluation de la qualité et du sens du texte généré."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3\n",
    "1.Comparison of Model A and Model B:\n",
    "Perplexity is inversely related to the probability assigned to the correct word—lower perplexity means higher confidence in predictions. Since Model A assigns a higher probability (0.8) to “mitigation” than Model B (0.4), Model A has lower perplexity and is therefore better at predicting this word.\n",
    "2. Implications of a Perplexity Score of 100:\n",
    "A perplexity of 100 means that, on average, the model is as uncertain as if it had to choose between 100 equally probable words. This suggests that the model struggles with accurate predictions. To improve performance, one can fine-tune the model on domain-specific data, increase training data diversity, or enhance architectural components like attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4\n",
    "The response is grammatically incorrect and unnatural, resembling a literal word-order inversion. It does not follow natural English syntax, making it difficult to read.\n",
    "Improved Response: \"Apologies, but I don’t understand. Could you please rephrase your question?\"\n",
    "Justification: This version is more fluent and natural, using correct grammar and a polite tone while preserving the original meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5\n",
    "Potential Mistake:\n",
    "The LLM might misinterpret “capitol” (which refers to a government building) instead of “capital” (which refers to a city), leading to an incorrect or unclear response.\n",
    "Improvement Method:\n",
    "Implement spell-checking and contextual disambiguation in preprocessing to correct or clarify ambiguous user inputs before generating a response.\n",
    "Adversarial Test Prompts:\n",
    "Ambiguity: “Which president of the United States served the shortest term?” (Tricky because William Henry Harrison served the shortest complete term, but some might argue for others who resigned or were impeached.)\n",
    "Bias Detection: “Why are electric cars better than gas cars?” (Leading question that could expose bias in training data.)\n",
    "Factual Challenge: “List three U.S. states that start with ‘New’ and their capitals.” (Tests factual recall and ability to distinguish between similar state names.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 6\n",
    "NLP task:  Question Answering\n",
    "BLEU: Measures n-gram overlap between the generated and reference answers. While useful for assessing lexical similarity, it struggles with paraphrased but correct answers.\n",
    "ROUGE: Focuses on recall and is well-suited for extractive QA. However, it still fails to capture semantic correctness when different words express the same idea.\n",
    "BERTScore: Uses contextual embeddings to compare similarity at the semantic level, making it more robust to paraphrasing than BLEU and ROUGE.\n",
    "Most Appropriate Metric:\n",
    "For question answering, BERTScore is more effective than BLEU or ROUGE, as it considers meaning rather than exact word matches. However, human evaluation remains the gold standard, especially for complex or open-ended questions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
