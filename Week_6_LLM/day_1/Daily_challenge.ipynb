{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess & Fine-Tune Transformer-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understanding BERT And XLM-RoBERTa \n",
    "BERT uses WordPiece tokenization, which breaks words down into subwords or word pieces.\n",
    "- BERT-Base: The standard version with 12 layers (transformer blocks), 768 hidden units, and 12 attention heads.\n",
    "- BERT-Large: A larger version with 24 layers, 1024 hidden units, and 16 attention heads.\n",
    "BERT is typically used for tasks like text classification, named entity recognition (NER), question answering (QA), and sentiment analysis.\n",
    "\n",
    "Like BERT, XLM-RoBERTa uses Byte-Pair Encoding (BPE) for tokenization, a technique that splits words into subword units. This helps the model handle various languages by creating a vocabulary that includes common subwords in all the languages it is trained on. XLM-RoBERTa is trained on 100+ languages.\n",
    "\n",
    "- XLM-RoBERTa-Base: 12 layers with 768 hidden units and 12 attention heads (similar to BERT-Base but multilingual).\n",
    "- XLM-RoBERTa-Large: 24 layers, 1024 hidden units, and 16 attention heads (similar to BERT-Large but multilingual).\n",
    "XLM-RoBERTa excels in tasks involving multiple languages, such as multilingual sentiment analysis, cross-lingual transfer learning, and translation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, XLMRobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized BERT output: {'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 2725, 2651, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Decoded BERT text: hello, how are you doing today?\n"
     ]
    }
   ],
   "source": [
    "# 2. Tokenizing Text\n",
    "# Load pre-trained BERT tokenizer \n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Hello, how are you doing today?\"\n",
    "\n",
    "tokenized_output_bert = tokenizer_bert.encode_plus(\n",
    "    text, \n",
    "    add_special_tokens=True,  # Adds [CLS] and [SEP] tokens\n",
    "    padding='max_length',     # Pad the sequences to a fixed length\n",
    "    max_length=10,            # Maximum sequence length\n",
    "    return_tensors='pt',      # Return PyTorch tensors\n",
    "    truncation=True           # Truncate if input exceeds max_length\n",
    ")\n",
    "\n",
    "# Output tokenized result\n",
    "print(\"Tokenized BERT output:\", tokenized_output_bert)\n",
    "\n",
    "# Decode the tokenized input back to text\n",
    "decoded_text_bert = tokenizer_bert.decode(tokenized_output_bert['input_ids'][0], skip_special_tokens=True)\n",
    "print(\"Decoded BERT text:\", decoded_text_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized XLM-RoBERTa output: {'input_ids': tensor([[    0,  1813, 18454,    38,  5187,  7843,    32,  1509, 21136, 81880,\n",
      "            38,     2,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n",
      "Decoded XLM-RoBERTa text: Привет! Как дела? Я тебя люблю!\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained XLM-RoBERTa tokenizer (xlm-roberta-base)\n",
    "tokenizer_xlm = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Sample text\n",
    "text_xlm = \"Привет! Как дела? Я тебя люблю!\"\n",
    "\n",
    "# Tokenizing the text using `encode_plus`\n",
    "tokenized_output_xlm = tokenizer_xlm.encode_plus(\n",
    "    text_xlm, \n",
    "    add_special_tokens=True,  # Adds [CLS] and [SEP] tokens\n",
    "    padding='max_length',     # Pad the sequences to a fixed length\n",
    "    max_length=15,            # Maximum sequence length\n",
    "    return_tensors='pt',      # Return PyTorch tensors\n",
    "    truncation=True           # Truncate if input exceeds max_length\n",
    ")\n",
    "\n",
    "# Output tokenized result\n",
    "print(\"Tokenized XLM-RoBERTa output:\", tokenized_output_xlm)\n",
    "\n",
    "# Decode the tokenized input back to text\n",
    "decoded_text_xlm = tokenizer_xlm.decode(tokenized_output_xlm['input_ids'][0], skip_special_tokens=True)\n",
    "print(\"Decoded XLM-RoBERTa text:\", decoded_text_xlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Special Tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "XLM-RoBERTa Special Tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n"
     ]
    }
   ],
   "source": [
    "# 3. Preparing Input Data For The Model\n",
    "# Special tokens\n",
    "print(\"BERT Special Tokens:\", tokenizer_bert.special_tokens_map)\n",
    "print(\"XLM-RoBERTa Special Tokens:\", tokenizer_xlm.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Vocabulary Size: 30522\n",
      "XLM-RoBERTa Vocabulary Size: 250002\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary Size\n",
    "print(\"BERT Vocabulary Size:\", tokenizer_bert.vocab_size)\n",
    "print(\"XLM-RoBERTa Vocabulary Size:\", tokenizer_xlm.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Input IDs: tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 2725, 2651, 1029,  102]])\n",
      "BERT Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "XLM-RoBERTa Input IDs: tensor([[    0,  1813, 18454,    38,  5187,  7843,    32,  1509, 21136, 81880,\n",
      "            38,     2,     1,     1,     1]])\n",
      "XLM-RoBERTa Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# Attention mask\n",
    "print(\"BERT Input IDs:\", tokenized_output_bert[\"input_ids\"])\n",
    "print(\"BERT Attention Mask:\", tokenized_output_bert[\"attention_mask\"])\n",
    "\n",
    "print(\"XLM-RoBERTa Input IDs:\", tokenized_output_xlm[\"input_ids\"])\n",
    "print(\"XLM-RoBERTa Attention Mask:\", tokenized_output_xlm[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Loading And Exploring The Dataset\n",
    "import pandas as pd\n",
    "train = pd.read_csv('/Users/patash/PSTB/Week_6_LLM/day_1/train.csv')\n",
    "test = pd.read_csv('/Users/patash/PSTB/Week_6_LLM/day_1/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id                                            premise  \\\n",
      "0  5130fd2cb5  and these comments were considered in formulat...   \n",
      "1  5b72532a0b  These are issues that we wrestle with in pract...   \n",
      "2  3931fbe82a  Des petites choses comme celles-là font une di...   \n",
      "3  5622f0c60b  you know they can't really defend themselves l...   \n",
      "4  86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n",
      "\n",
      "                                          hypothesis lang_abv language  label  \n",
      "0  The rules developed in the interim were put to...       en  English      0  \n",
      "1  Practice groups are not permitted to work on t...       en  English      2  \n",
      "2              J'essayais d'accomplir quelque chose.       fr   French      0  \n",
      "3  They can't defend themselves because of their ...       en  English      0  \n",
      "4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai      1  \n",
      "           id                                            premise  \\\n",
      "0  c6d58c3f69  بکس، کیسی، راہیل، یسعیاہ، کیلی، کیلی، اور کولم...   \n",
      "1  cefcc82292                             هذا هو ما تم نصحنا به.   \n",
      "2  e98005252c  et cela est en grande partie dû au fait que le...   \n",
      "3  58518c10ba                   与城市及其他公民及社区组织代表就IMA的艺术发展进行对话&amp   \n",
      "4  c32b0d16df                              Она все еще была там.   \n",
      "\n",
      "                                          hypothesis lang_abv language  \n",
      "0  کیسی کے لئے کوئی یادگار نہیں ہوگا, کولمین ہائی...       ur     Urdu  \n",
      "1  عندما يتم إخبارهم بما يجب عليهم فعله ، فشلت ال...       ar   Arabic  \n",
      "2                             Les mères se droguent.       fr   French  \n",
      "3                            IMA与其他组织合作，因为它们都依靠共享资金。       zh  Chinese  \n",
      "4     Мы думали, что она ушла, однако, она осталась.       ru  Russian  \n"
     ]
    }
   ],
   "source": [
    "print(train.head())\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12120, 6)\n",
      "(5195, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             premise  \\\n",
      "0  and these comments were considered in formulat...   \n",
      "1  These are issues that we wrestle with in pract...   \n",
      "2  Des petites choses comme celles-là font une di...   \n",
      "3  you know they can't really defend themselves l...   \n",
      "4  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n",
      "\n",
      "                                          hypothesis  label  \n",
      "0  The rules developed in the interim were put to...      0  \n",
      "1  Practice groups are not permitted to work on t...      2  \n",
      "2              J'essayais d'accomplir quelque chose.      0  \n",
      "3  They can't defend themselves because of their ...      0  \n",
      "4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร      1  \n"
     ]
    }
   ],
   "source": [
    "df_train_filtered = train[['premise', 'hypothesis', 'label']].copy()\n",
    "\n",
    "print(df_train_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             premise  \\\n",
      "0  and these comments were considered in formulat...   \n",
      "1  These are issues that we wrestle with in pract...   \n",
      "2  Des petites choses comme celles-là font une di...   \n",
      "3  you know they can't really defend themselves l...   \n",
      "4  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n",
      "\n",
      "                                          hypothesis  label  \\\n",
      "0  The rules developed in the interim were put to...      0   \n",
      "1  Practice groups are not permitted to work on t...      2   \n",
      "2              J'essayais d'accomplir quelque chose.      0   \n",
      "3  They can't defend themselves because of their ...      0   \n",
      "4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร      1   \n",
      "\n",
      "             premise_tokenized         hypothesis_tokenized  \\\n",
      "0  [input_ids, attention_mask]  [input_ids, attention_mask]   \n",
      "1  [input_ids, attention_mask]  [input_ids, attention_mask]   \n",
      "2  [input_ids, attention_mask]  [input_ids, attention_mask]   \n",
      "3  [input_ids, attention_mask]  [input_ids, attention_mask]   \n",
      "4  [input_ids, attention_mask]  [input_ids, attention_mask]   \n",
      "\n",
      "                                   premise_input_ids  \\\n",
      "0  [0, 136, 6097, 24626, 3542, 90698, 23, 26168, ...   \n",
      "1  [0, 32255, 621, 37348, 450, 642, 148, 56644, 1...   \n",
      "2  [0, 5581, 69332, 37899, 3739, 91362, 9, 16161,...   \n",
      "3  [0, 398, 3714, 1836, 831, 25, 18, 6183, 65922,...   \n",
      "4  [0, 6976, 114538, 171936, 18379, 101830, 14435...   \n",
      "\n",
      "                              premise_attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                hypothesis_input_ids  \\\n",
      "0  [0, 581, 91736, 126809, 23, 70, 1940, 464, 354...   \n",
      "1  [0, 109613, 13, 94407, 621, 959, 28897, 3674, ...   \n",
      "2  [0, 821, 25, 6011, 395, 164, 104, 25, 2263, 58...   \n",
      "3  [0, 10660, 831, 25, 18, 65922, 61261, 6637, 11...   \n",
      "4  [0, 86094, 14762, 19575, 82544, 46732, 75470, ...   \n",
      "\n",
      "                           hypothesis_attention_mask  \n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...  \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# xlm-roberta-base\n",
    "tokenizer_xlm = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "# Tokenisation des colonnes 'premise' et 'hypothesis'\n",
    "\n",
    "df_train_filtered['premise_tokenized'] = df_train_filtered['premise'].apply(\n",
    "    lambda x: tokenizer_xlm.encode_plus(\n",
    "        x,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    ")\n",
    "\n",
    "df_train_filtered['hypothesis_tokenized'] = df_train_filtered['hypothesis'].apply(\n",
    "    lambda x: tokenizer_xlm.encode_plus(\n",
    "        x,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    ")\n",
    "\n",
    "# Extraire les input_ids et attention_mask séparément\n",
    "df_train_filtered['premise_input_ids'] = df_train_filtered['premise_tokenized'].apply(lambda x: x['input_ids'])\n",
    "df_train_filtered['premise_attention_mask'] = df_train_filtered['premise_tokenized'].apply(lambda x: x['attention_mask'])\n",
    "\n",
    "df_train_filtered['hypothesis_input_ids'] = df_train_filtered['hypothesis_tokenized'].apply(lambda x: x['input_ids'])\n",
    "df_train_filtered['hypothesis_attention_mask'] = df_train_filtered['hypothesis_tokenized'].apply(lambda x: x['attention_mask'])\n",
    "\n",
    "# Afficher les 5 premières lignes\n",
    "print(df_train_filtered.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Conversion en tensors\n",
    "premise_input_ids = torch.tensor(df_train_filtered['premise_input_ids'].tolist())\n",
    "hypothesis_input_ids = torch.tensor(df_train_filtered['hypothesis_input_ids'].tolist())\n",
    "premise_attention_mask = torch.tensor(df_train_filtered['premise_attention_mask'].tolist())\n",
    "hypothesis_attention_mask = torch.tensor(df_train_filtered['hypothesis_attention_mask'].tolist())\n",
    "labels = torch.tensor(df_train_filtered['label'].tolist())\n",
    "\n",
    "# Dataset PyTorch avec 4 entrées : premise_input_ids, hypothesis_input_ids, premise_attention_mask, hypothesis_attention_mask, et labels\n",
    "dataset = TensorDataset(premise_input_ids, hypothesis_input_ids, premise_attention_mask, hypothesis_attention_mask, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Création de StratifiedKFold pour générer 5 splits\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Listes pour stocker les DataLoader d'entraînement et de validation pour chaque pli\n",
    "train_dataloaders = []\n",
    "val_dataloaders = []\n",
    "\n",
    "# Appliquer les indices de StratifiedKFold et créer les DataLoader\n",
    "for train_index, val_index in kf.split(premise_input_ids, labels):\n",
    "    # Créer des sous-ensembles du dataset pour l'entraînement et la validation\n",
    "    train_subset = TensorDataset(\n",
    "        premise_input_ids[train_index],\n",
    "        hypothesis_input_ids[train_index],\n",
    "        premise_attention_mask[train_index],\n",
    "        hypothesis_attention_mask[train_index],\n",
    "        labels[train_index]\n",
    "    )\n",
    "    \n",
    "    val_subset = TensorDataset(\n",
    "        premise_input_ids[val_index],\n",
    "        hypothesis_input_ids[val_index],\n",
    "        premise_attention_mask[val_index],\n",
    "        hypothesis_attention_mask[val_index],\n",
    "        labels[val_index]\n",
    "    )\n",
    "    \n",
    "    # Créer des DataLoader pour les sous-ensembles\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Ajouter les DataLoader aux listes\n",
    "    train_dataloaders.append(train_dataloader)\n",
    "    val_dataloaders.append(val_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
