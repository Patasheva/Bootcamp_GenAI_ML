{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises 1.1\n",
    "Traditional NLP: \n",
    "- Feature Engineering Manual (TF-IDF, Bag-of-Words, n-grams)\t\n",
    "- Word Representations Static (Word2Vec, GloVe)\t\n",
    "- Model Architectures Shallow (Naïve Bayes, SVM, Logistic Regression)\t\n",
    "- Training Methodology Task-specific (trained on individual tasks)\t\n",
    "- Naïve Bayes, SVM, HMM, Word2Vec\t\n",
    "- Advantages: Requires less computational power + More interpretable models\t\n",
    "- Disadvantages Poor handling of polysemy and context + Requires extensive feature engineering\n",
    "Modern NLP: \n",
    "- Feature Engineering Automatic (learned representations)\n",
    "- Word Representations Contextual (BERT, GPT, Transformer-based)\n",
    "- Model Architectures Deep (Transformers, LSTMs, CNNs)\n",
    "- Pre-training + Fine-tuning (transfer learning)\n",
    "- BERT, GPT, T5, RoBERTa, ChatGPT\n",
    "- Advantages: Handles context better + Generalizes across multiple tasks\n",
    "- Desadvantaged:  High computational cost + Requires large labeled datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Exercise 1.2\n",
    "The evolution from traditional to modern NLP has drastically improved the scalability and efficiency of NLP applications. Traditional methods, reliant on manual feature engineering and shallow models, were often limited by the amount of human effort required and struggled to scale across tasks. Modern NLP, with its deep learning models like transformers, leverages large pre-trained models that can be fine-tuned for various tasks, enabling broader application coverage with minimal task-specific adjustments. This shift has allowed NLP systems to handle massive datasets and complex contexts, leading to improved performance across a wide range of applications, from machine translation to sentiment analysis. However, the trade-off is the increased computational cost and resource requirements, which can impact scalability in resource-constrained environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Exercise 2\n",
    "1. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "Core Architectural Differences:\n",
    "Bidirectional: BERT uses a bidirectional approach, meaning it considers the entire context of a sentence (both the left and right side of a word) during training. This is achieved through the masked language modeling (MLM) approach.\n",
    "Masked Language Modeling (MLM): Instead of predicting the next word, BERT randomly masks some of the words in the input and trains the model to predict the missing words based on their surrounding context.\n",
    "Real-World Application:\n",
    "Question Answering (QA): BERT is particularly well-suited for question-answering tasks, such as those in the SQuAD dataset (Stanford Question Answering Dataset).\n",
    "Why BERT is Well-Suited for QA:\n",
    "The bidirectional nature allows BERT to capture context from both directions in a sentence, making it effective in understanding nuanced questions and extracting the correct answer from a passage. Since the model comprehends the full context of both the question and the passage, it can perform better in tasks that require contextual understanding, such as selecting the relevant sentence or phrase to answer a question.\n",
    "2. GPT (Generative Pre-trained Transformer)\n",
    "Core Architectural Differences:\n",
    "Unidirectional: GPT uses a unidirectional (left-to-right) approach, meaning it generates text by predicting the next word based on the preceding words.\n",
    "Causal Language Modeling: GPT is trained to predict the next word in a sequence given the previous words (causal language modeling), which helps in generating coherent and contextually appropriate sentences.\n",
    "Real-World Application:\n",
    "Text Generation: GPT excels in tasks like content creation, code generation, and chatbot responses. One of its prominent applications is in generating human-like text for creative writing or answering open-ended questions.\n",
    "Why GPT is Well-Suited for Text Generation:\n",
    "The unidirectional nature of GPT allows it to generate text word by word, ensuring that the generated text follows a coherent flow from left to right. The model’s ability to generate text in a natural, flowing manner makes it ideal for applications where creativity and context continuation are needed, such as writing essays, stories, or providing coherent dialogue in chatbots.\n",
    "3. T5 (Text-to-Text Transfer Transformer)\n",
    "Core Architectural Differences:\n",
    "Text-to-Text Framework: T5 treats every NLP problem as a text-to-text task, meaning it converts all tasks (e.g., translation, summarization, question answering) into a format where both the input and output are text sequences.\n",
    "Unified Model: It uses a sequence-to-sequence (seq2seq) architecture, where both the encoder and decoder are based on transformers. T5 can perform a wide range of tasks by conditioning the model with specific task instructions.\n",
    "Real-World Application:\n",
    "Text Summarization: T5 excels in abstractive and extractive summarization tasks, where it can generate summaries from long pieces of text.\n",
    "Why T5 is Well-Suited for Summarization:\n",
    "The seq2seq architecture allows T5 to process input text and generate summaries effectively. Additionally, the text-to-text format allows for task-specific conditioning, which enables T5 to generate summaries by treating the task as a transformation of the input text (e.g., “summarize: {text}”). This flexibility makes T5 a powerful tool for transforming long text into concise summaries.\n",
    "In summary, BERT is ideal for tasks requiring deep contextual understanding of a text (like question answering), GPT is best for generative tasks (like text creation), and T5 is highly versatile for tasks that involve transforming text into another form (like summarization). Each architecture is optimized for specific types of tasks based on its unique strengths in processing language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Exercise 5\n",
    "\n",
    "DistilBERT: Real-time sentiment analysis on mobile app with limited resources. DistilBERT is a smaller, faster version of BERT that retains most of the original model's accuracy. It is more suitable for real-time applications on mobile apps due to its smaller size and efficiency.\n",
    "RoBERTa: Research on legal documents requiring high accuracy. RoBERTa is optimized for better performance by training on larger datasets with more epochs. This makes it ideal for tasks where accuracy is paramount, such as legal document analysis, where subtle meanings and precision are critical.\n",
    "XLM-RoBERTa: Global customer support in multiple languages. XLM-RoBERTa is a multilingual variant of RoBERTa and excels in handling a variety of languages. It is specifically trained on many languages, making it ideal for multilingual customer support tasks.\n",
    "ELECTRA: Efficient pretraining, and token replacement detection.\tELECTRA uses a generator-discriminator setup where the model learns by detecting token replacements. This allows for more efficient pretraining and faster convergence, making it ideal for tasks involving token replacement detection.\n",
    "ALBERT: Efficient NLP in resource-constrained environments. ALBERT is a lighter version of BERT with parameter-sharing techniques that reduce model size and memory usage while retaining high performance. It is well-suited for resource-constrained environments like edge devices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
