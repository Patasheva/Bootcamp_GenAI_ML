{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building A GAN-Based AI Text Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1378, 4)\n",
      "(3, 3)\n",
      "(2, 4)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/Users/patash/PSTB/Week_6_LLM/day_4/llm-detect-ai-generated-text/train_essays.csv')\n",
    "test = pd.read_csv('/Users/patash/PSTB/Week_6_LLM/day_4/llm-detect-ai-generated-text/test_essays.csv')\n",
    "prompt = pd.read_csv('/Users/patash/PSTB/Week_6_LLM/day_4/llm-detect-ai-generated-text/train_prompts.csv')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(prompt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  prompt_id                                               text  \\\n",
      "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
      "1  005db917          0  Transportation is a large necessity in most co...   \n",
      "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
      "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
      "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
      "\n",
      "   generated  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n"
     ]
    }
   ],
   "source": [
    "print(train.head())\n",
    "# prompt_id: prompt utilisé pour générer le texte 0 ou 1\n",
    "# text: texte généré ou humain \n",
    "# generated: 1 = généré par IA, 0 = humain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  prompt_id          text\n",
      "0  0000aaaa          2  Aaa bbb ccc.\n",
      "1  1111bbbb          3  Bbb ccc ddd.\n",
      "2  2222cccc          4  CCC ddd eee.\n"
     ]
    }
   ],
   "source": [
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prompt_id                       prompt_name  \\\n",
      "0          0                   Car-free cities   \n",
      "1          1  Does the electoral college work?   \n",
      "\n",
      "                                        instructions  \\\n",
      "0  Write an explanatory essay to inform fellow ci...   \n",
      "1  Write a letter to your state senator in which ...   \n",
      "\n",
      "                                         source_text  \n",
      "0  # In German Suburb, Life Goes On Without Cars ...  \n",
      "1  # What Is the Electoral College? by the Office...  \n"
     ]
    }
   ],
   "source": [
    "print(prompt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1378 entries, 0 to 1377\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         1378 non-null   object\n",
      " 1   prompt_id  1378 non-null   int64 \n",
      " 2   text       1378 non-null   object\n",
      " 3   generated  1378 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 43.2+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   id         3 non-null      object\n",
      " 1   prompt_id  3 non-null      int64 \n",
      " 2   text       3 non-null      object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 204.0+ bytes\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   prompt_id     2 non-null      int64 \n",
      " 1   prompt_name   2 non-null      object\n",
      " 2   instructions  2 non-null      object\n",
      " 3   source_text   2 non-null      object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 196.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train.info())\n",
    "print(test.info())\n",
    "print(prompt.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model preparation\n",
    "\n",
    "model = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model)\n",
    "pretrained_model = BertForSequenceClassification.from_pretrained(model, num_labels=2)\n",
    "embedding_model = pretrained_model  \n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "train_batch_size = 32\n",
    "test_batch_size = 32\n",
    "lr = 0.0002\n",
    "beta1 = 0.9\n",
    "nz = 100  # Dimensions of the latent vector\n",
    "num_epochs = 3\n",
    "num_hidden_layers = 3\n",
    "train_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "class GANDAIGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "all_num = len(train) \n",
    "train_num = int(all_num * train_ratio)\n",
    "test_num = all_num - train_num\n",
    "\n",
    "# Train - 80%, test - 20%\n",
    "train_set = train.iloc[:train_num] # Les premières 80% pour l’entraînement\n",
    "test_set = pd.concat([\n",
    "    train.iloc[train_num:],  \n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# Data GAN \n",
    "train_dataset = GANDAIGDataset(train_set['text'], train_set['generated'])\n",
    "test_dataset = GANDAIGDataset(test_set['text'], test_set['generated'])\n",
    "\n",
    "# Loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, train_batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, test_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator \n",
    "config = BertConfig(num_hidden_layers=num_hidden_layers)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, 256 * 128)\n",
    "\n",
    "        self.conv_net = nn.Sequential(\n",
    "           nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.Conv1d(in_channels=512, out_channels=768, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    "           nn.Conv1d(in_channels=768, out_channels=768, kernel_size=3, stride=1, padding=1),\n",
    "           nn.ReLU(),\n",
    ")\n",
    "        self.bert_encoder = BertEncoder(config)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)  # Transformer le bruit latent en une représentation initiale\n",
    "        x = x.view(-1, 256, 128)  # Reshape pour correspondre aux dimensions attendues par le CNN\n",
    "        x = self.conv_net(x)  # Passage dans le réseau convolutionnel\n",
    "        x = x.permute(0, 2, 1)  # Adapter le format pour BERT (batch_size, seq_len, hidden_dim)\n",
    "    \n",
    "        encoder_outputs = self.bert_encoder(x)  # Passage dans l'encodeur BERT\n",
    "    \n",
    "        return encoder_outputs.last_hidden_state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class SumBertPooler(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        sum_hidden = hidden_states.sum(dim=1)\n",
    "        sum_mask = sum_hidden.sum(1).unsqueeze(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "\n",
    "        mean_embeddings = sum_hidden / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert_encoder = BertEncoder(config)\n",
    "        self.bert_encoder.layer = nn.ModuleList([\n",
    "            layer for layer in pretrained_model.bert.encoder.layer[:6]\n",
    "        ])\n",
    "        self.pooler = SumBertPooler()\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, 256),  # Réduction de la dimension des embeddings BERT\n",
    "            nn.ReLU(),  # Activation non linéaire\n",
    "            nn.Dropout(0.1),  # Régularisation pour éviter le sur-apprentissage\n",
    "            nn.Linear(256, 1)  # Projection finale vers une seule sortie (logit)\n",
    ")\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.bert_encoder(input)\n",
    "        out = self.pooler(out.last_hidden_state)\n",
    "        out = self.classifier(out)\n",
    "        return torch.sigmoid(out).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# % (epoch, num_epochs, i, len(train_loader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimizerG, optimizerD, netG, netD\n\u001b[0;32m---> 90\u001b[0m netG \u001b[38;5;241m=\u001b[39m Generator(nz)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     91\u001b[0m netD \u001b[38;5;241m=\u001b[39m Discriminator()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     93\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss() \n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def eval_auc(model):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            encodings = tokenizer(batch[0], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            input_ids = encodings[\"input_ids\"]\n",
    "            token_type_ids = encodings[\"token_type_ids\"]\n",
    "            attention_mask = encodings[\"attention_mask\"]\n",
    "            \n",
    "            embeded = embedding_model(input_ids=input_ids, \n",
    "                                      token_type_ids=token_type_ids, \n",
    "                                      attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "            label = batch[1].float().to(device)\n",
    "\n",
    "            outputs = model(embeded)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(label.cpu().numpy())\n",
    "\n",
    "    auc = roc_auc_score(actuals, predictions)\n",
    "    print(\"AUC:\", auc)\n",
    "    return auc\n",
    "\n",
    "# 1. encode le texte avec le tokenizer.\n",
    "# 2. passe ces encodages au modèle de classification (embedding_model).\n",
    "# 3. récupère l'output et on applique AUC-ROC via roc_auc_score.\n",
    "\n",
    "def get_model_info_dict(model, epoch, auc_score):\n",
    "    # Récupère l'appareil sur lequel le modèle est actuellement (par exemple, CPU ou GPU)\n",
    "    current_device = next(model.parameters()).device\n",
    "    \n",
    "    # Déplace temporairement le modèle sur le CPU pour sauvegarder son état\n",
    "    model.to('cpu')\n",
    "\n",
    "    # Crée un dictionnaire contenant les informations clés pour ce checkpoint\n",
    "    model_info = {\n",
    "        'epoch': epoch,  # L'époque en cours\n",
    "        'model_state_dict': model.state_dict(),  # L'état actuel du modèle (poids, biais, etc.)\n",
    "        'auc_score': auc_score,  # Le score AUC du modèle pour cette époque\n",
    "    }\n",
    "\n",
    "    # Restaure le modèle sur son appareil d'origine (GPU ou CPU)\n",
    "    model.to(current_device)\n",
    "    \n",
    "    # Retourne le dictionnaire des informations\n",
    "    return model_info\n",
    "\n",
    "def preparation_embedding(texts):\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = encodings['input_ids']\n",
    "    token_type_ids = encodings['token_type_ids']\n",
    "    embeded = embedding_model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "    return embeded\n",
    "\n",
    "def GAN_step(optimizerG, optimizerD, netG, netD, real_data, label, epoch, i):\n",
    "    netD.zero_grad()\n",
    "    batch_size = real_data.size(0)\n",
    "\n",
    "    output = netD(real_data)\n",
    "    errD_real = criterion(output, label)\n",
    "    errD_real.backward()\n",
    "    D_x = output.mean().item()\n",
    "\n",
    "    noise = torch.randn(batch_size, nz, device=device)\n",
    "    fake_data = netG(noise).last_hidden_state\n",
    "    label.fill_(1)\n",
    "    output = netD(fake_data.detach())\n",
    "    errD_fake = criterion(output, label)\n",
    "    errD_fake.backward()\n",
    "    D_G_z1 = output.mean().item()\n",
    "    errD = errD_real + errD_fake\n",
    "    optimizerD.step()\n",
    "\n",
    "    netG.zero_grad()\n",
    "    label.fill_(0)\n",
    "    output = netD(fake_data)\n",
    "    errG = criterion(output, label)\n",
    "    errG.backward()\n",
    "    D_G_z2 = output.mean().item()\n",
    "    optimizerG.step()\n",
    "    if i % 50 == 0:\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f')\n",
    "# % (epoch, num_epochs, i, len(train_loader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "    return optimizerG, optimizerD, netG, netD\n",
    "\n",
    "netG = Generator(nz).to(device)\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "criterion = nn.BCELoss() \n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Stockage des informations du modèle\n",
    "model_infos = []\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        with torch.no_grad():\n",
    "            embeded = preparation_embedding(data[0])\n",
    "\n",
    "        optimizerG, optimizerD, netG, netD = GAN_step(\n",
    "            optimizerG=optimizerG,\n",
    "            optimizerD=optimizerD,\n",
    "            netG=netG,\n",
    "            netD=netD,\n",
    "            real_data=embeded.to(device),\n",
    "            label=data[1].float().to(device),\n",
    "            epoch=epoch, i=i)\n",
    "\n",
    "    auc_score = eval_auc(netD)  \n",
    "    model_infos.append(get_model_info_dict(netD, epoch, auc_score))\n",
    "\n",
    "print('Train complete！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "# Chargement du modèle avec les meilleurs paramètres\n",
    "max_auc_model_info = torch.load('chemin/vers/le/meilleur_modele.pth')  # Remplacez par le bon chemin\n",
    "model = Discriminator()\n",
    "model.load_state_dict(max_auc_model_info['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Dataset pour l'inférence\n",
    "class InferenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "# Remplissez ici avec les textes à prédire (par exemple, une liste de textes ou une colonne d'un DataFrame)\n",
    "sub_dataset = InferenceDataset(texts_to_predict)  # Remplacez 'texts_to_predict' par votre liste de textes\n",
    "\n",
    "# DataLoader pour l'inférence\n",
    "inference_loader = torch.utils.data.DataLoader(sub_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Liste pour stocker les prédictions\n",
    "sub_predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in inference_loader:\n",
    "        # Tokenisation des textes\n",
    "        encodings = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        token_type_ids = encodings['token_type_ids'].to(device)\n",
    "\n",
    "        # Extraction des embeddings du modèle\n",
    "        embeded = embedding_model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "        # Passer les embeddings à travers le modèle\n",
    "        embeded = embeded.to(device)\n",
    "        outputs = model(embeded)\n",
    "        \n",
    "        # Collecte des prédictions\n",
    "        sub_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "# Conversion des résultats en DataFrame ou autre format nécessaire\n",
    "sub_ans_df = pd.DataFrame(sub_predictions, columns=['prediction'])  # Remplacez 'prediction' par le nom de votre colonne\n",
    "\n",
    "# Afficher les résultats\n",
    "print(sub_ans_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
